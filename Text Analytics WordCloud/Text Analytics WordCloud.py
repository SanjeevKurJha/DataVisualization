# -*- coding: utf-8 -*-
"""WordCloud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O06NJMLdj2LaQ9a1msX6xNnSwsLxXm5m

**Importing the Library**
"""

import bs4 as bs
import numpy as np
import pandas as  pd
import nltk
import time
import collections
nltk.download('punkt')
import re
import matplotlib.pyplot as plt
from wordcloud import WordCloud
nltk.download('inaugural')
from nltk.tokenize import word_tokenize,sent_tokenize 
import urllib.request
nltk.download('stopwords')
from nltk.corpus import stopwords

"""**Importing the text data**"""

text_data = "India will celebrate its 70th Republic Day on January 26 (Saturday). Republic Day is celebrated with great enthusiasm across the country including in every school, college, government and private institutions to honour the Constitution that came to force on January 26 in 1950.The day is a national holiday in India. Several events are organised in different states to mark the day. In schools, colleges and other educational institutions, several cultural programmes, speech recitation, group discussion are organised by students to demonstrate Indiaâ€™s rich heritage and culture."

"""**Visulaise the text data**"""

text_data

"""**Function for cleaning the data and removing the stops words**"""

Replace_by_Space_re = re.compile('[/(){}\[\]\|@,;]')
Bad_Symbols_Re = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))
def text_prepare(sentances):
  print(sentances)
  #for text in sentances: 
  text_data = text.lower()
  text_data = re.sub(Replace_by_Space_re, ' ', text_data)
  text_data = re.sub(Bad_Symbols_Re,'',text_data)
  text_data = " ".join([word for word in sent_tokenize(text_data) if not word in STOPWORDS]) 
  return text_data

"""**Call the text prepare function**"""

text_prepare_results=text_prepare(text_data)

"""**Visulaize the text prepare data**"""

text_prepare_results

word_tokens = nltk.word_tokenize(text_prepare_results)
word_tokens

# Dictionary of all words from train corpus with their counts.
words_counts = {}
for word in word_tokens:
  if word in words_counts:
    words_counts[word] = words_counts[word] + 1
  else:
    words_counts[word] = 1 
words_counts

"""**getting the top three most popular word counts**"""

most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:3]
print(most_common_words)

# Commented out IPython magic to ensure Python compatibility.
import operator
# %matplotlib inline
Freq_dist_nltk=nltk.FreqDist(word_tokens)
print(Freq_dist_nltk)
sorted_d = sorted(Freq_dist_nltk.items(), key=operator.itemgetter(1),reverse=True)
print(sorted_d[:25])
Freq_dist_nltk.plot(25, cumulative=False)

# Building the wordcloud
from wordcloud import WordCloud
wordcloud = WordCloud().generate(text_data)
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

def plot_wordcloud(wordcloud):
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.show()
wordcloud = WordCloud().generate(text)
plot_wordcloud(wordcloud)
wordcloud = WordCloud(stopwords={'to','of'}).generate(text_data)
plot_wordcloud(wordcloud)

# Building the wordcloud with relative scaling = 0
wordcloud = WordCloud(relative_scaling = 0, stopwords = {'to','of'}).generate(text_data)
plot_wordcloud(wordcloud)

# Building the wordcloud with relative scaling = 1
wordcloud = WordCloud(relative_scaling = 1.0, stopwords = {'to','of'}).generate(text_data)
plot_wordcloud(wordcloud)

"""**Using [Url](https://www.moneycontrol.com/news/india/independence-day-speech-modi-is-a-man-in-a-hurry-can-india-run-with-him-4338721.html)**"""

df = urllib.request.urlopen("https://www.moneycontrol.com/news/india/independence-day-speech-modi-is-a-man-in-a-hurry-can-india-run-with-him-4338721.html")

soup = bs.BeautifulSoup(df,'html')
text_data = ""
for paragraph in soup.find_all('p'):
    text_data += paragraph.text

text_prepare_URLresults=text_prepare(text_data)

text_prepare_URLresults

word_tokens = nltk.word_tokenize(text_prepare_URLresults)
word_tokens

# Dictionary of all words from train corpus with their counts.
words_counts = {}
for word in word_tokens:
  if word in words_counts:
    words_counts[word] = words_counts[word] + 1
  else:
    words_counts[word] = 1 
words_counts

most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:3]
print(most_common_words)

# Commented out IPython magic to ensure Python compatibility.
import operator
# %matplotlib inline
Freq_dist_nltk=nltk.FreqDist(word_tokens)
print(Freq_dist_nltk)
sorted_d = sorted(Freq_dist_nltk.items(), key=operator.itemgetter(1),reverse=True)
print(sorted_d[:25])
Freq_dist_nltk.plot(25, cumulative=False)

# Building the wordcloud
from wordcloud import WordCloud
wordcloud = WordCloud().generate(text_data)
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

plot_wordcloud(wordcloud)

# Building the wordcloud with relative scaling = 0
wordcloud = WordCloud(relative_scaling = 0, stopwords = {'to','of'}).generate(text_data)
plot_wordcloud(wordcloud)

# Building the wordcloud with relative scaling = 1
wordcloud = WordCloud(relative_scaling = 1.0, stopwords = {'to','of'}).generate(text_data)
plot_wordcloud(wordcloud)

